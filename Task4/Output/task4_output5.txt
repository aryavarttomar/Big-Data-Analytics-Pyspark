++ id -u
+ myuid=1000770000
++ id -g
+ mygid=0
+ set +e
++ getent passwd 1000770000
+ uidentry='1000770000:x:1000770000:0:1000770000 user:/home/jboss:/sbin/nologin'
+ set -e
+ '[' -z '1000770000:x:1000770000:0:1000770000 user:/home/jboss:/sbin/nologin' ']'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/opt/hadoop/share/hadoop/tools/lib/*' ']'
+ SPARK_CLASSPATH=':/opt/spark/jars/*:/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/opt/hadoop/share/hadoop/tools/lib/*'
+ '[' '' == 2 ']'
+ '[' '' == 3 ']'
+ '[' -n /opt/hadoop ']'
+ '[' -z '/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/opt/hadoop/share/hadoop/tools/lib/*' ']'
+ '[' -z ']'
+ case "$1" in
+ shift 1
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.133.0.5 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner s3a://object-bucket-ec24848-8bb6712d-c99d-4727-a9d6-d67ec41345d8/spark-hs/spark-upload-61522581-7bee-4765-b54e-6f278b93238f/task4.py
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.hadoop#hadoop-aws added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-b262285e-c630-4041-bfd1-5bbdae5f6515;1.0
	confs: [default]
	found org.apache.hadoop#hadoop-aws;3.2.2 in central
	found com.amazonaws#aws-java-sdk-bundle;1.11.563 in central
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.2/hadoop-aws-3.2.2.jar ...
	[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.2!hadoop-aws.jar (22ms)
downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar ...
	[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.563!aws-java-sdk-bundle.jar (1293ms)
:: resolution report :: resolve 1330ms :: artifacts dl 1318ms
	:: modules in use:
	com.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]
	org.apache.hadoop#hadoop-aws;3.2.2 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-b262285e-c630-4041-bfd1-5bbdae5f6515
	confs: [default]
	2 artifacts copied, 0 already retrieved (127385kB/66ms)
2025-04-10 03:24:35,814 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-10 03:24:38,234 INFO spark.SparkContext: Running Spark version 3.0.1
2025-04-10 03:24:38,273 INFO resource.ResourceUtils: ==============================================================
2025-04-10 03:24:38,274 INFO resource.ResourceUtils: Resources for spark.driver:

2025-04-10 03:24:38,275 INFO resource.ResourceUtils: ==============================================================
2025-04-10 03:24:38,275 INFO spark.SparkContext: Submitted application: StreamingHDFSLogs_Task
2025-04-10 03:24:38,340 INFO spark.SecurityManager: Changing view acls to: 1000770000,ec24848
2025-04-10 03:24:38,340 INFO spark.SecurityManager: Changing modify acls to: 1000770000,ec24848
2025-04-10 03:24:38,340 INFO spark.SecurityManager: Changing view acls groups to: 
2025-04-10 03:24:38,341 INFO spark.SecurityManager: Changing modify acls groups to: 
2025-04-10 03:24:38,341 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1000770000, ec24848); groups with view permissions: Set(); users  with modify permissions: Set(1000770000, ec24848); groups with modify permissions: Set()
2025-04-10 03:24:38,615 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.
2025-04-10 03:24:38,647 INFO spark.SparkEnv: Registering MapOutputTracker
2025-04-10 03:24:38,682 INFO spark.SparkEnv: Registering BlockManagerMaster
2025-04-10 03:24:38,703 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-10 03:24:38,703 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-04-10 03:24:38,707 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
2025-04-10 03:24:38,723 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-cd775f5b-a747-4ba0-97a6-62f99e525ae9/blockmgr-0ee98d13-40ca-4828-8b93-bcd65d363717
2025-04-10 03:24:38,750 INFO memory.MemoryStore: MemoryStore started with capacity 2004.6 MiB
2025-04-10 03:24:38,769 INFO spark.SparkEnv: Registering OutputCommitCoordinator
2025-04-10 03:24:38,883 INFO util.log: Logging initialized @7521ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-10 03:24:38,960 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_332-b09
2025-04-10 03:24:38,984 INFO server.Server: Started @7622ms
2025-04-10 03:24:39,023 INFO server.AbstractConnector: Started ServerConnector@46873f8b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2025-04-10 03:24:39,024 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
2025-04-10 03:24:39,049 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45cff3d2{/jobs,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,052 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7945b421{/jobs/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,052 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3301f156{/jobs/job,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,054 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6eab6e77{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,054 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a465db8{/stages,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,055 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31b0e7a9{/stages/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,056 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e4517d4{/stages/stage,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,057 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4281b413{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,057 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d107f50{/stages/pool,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,058 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51361561{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,058 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b944493{/storage,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,059 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14326812{/storage/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,060 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a502941{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,060 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fdf856d{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,061 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5dd5b890{/environment,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,061 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@76c47a1e{/environment/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,062 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64ca999a{/executors,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,063 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fdf0d31{/executors/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,063 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@461e7f71{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,065 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f981843{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,074 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d24d159{/static,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,074 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@797e014c{/,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,075 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@630d46f3{/api,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,076 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@613069fb{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,076 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a10c5a5{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-10 03:24:39,079 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://task4-fa4e6d961dba3a3d-driver-svc.data-science-ec24848.svc:4040
2025-04-10 03:24:39,110 INFO spark.SparkContext: Added JAR local:///opt/spark/jars/graphframes-0.8.2-spark3.0-s_2.12.jar at file:/opt/spark/jars/graphframes-0.8.2-spark3.0-s_2.12.jar with timestamp 1744255479110
2025-04-10 03:24:39,204 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
2025-04-10 03:24:40,098 INFO k8s.ExecutorPodsAllocator: Going to request 2 executors from Kubernetes.
2025-04-10 03:24:40,112 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
2025-04-10 03:24:40,113 INFO netty.NettyBlockTransferService: Server created on task4-fa4e6d961dba3a3d-driver-svc.data-science-ec24848.svc:7079
2025-04-10 03:24:40,114 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-10 03:24:40,123 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, task4-fa4e6d961dba3a3d-driver-svc.data-science-ec24848.svc, 7079, None)
2025-04-10 03:24:40,128 INFO storage.BlockManagerMasterEndpoint: Registering block manager task4-fa4e6d961dba3a3d-driver-svc.data-science-ec24848.svc:7079 with 2004.6 MiB RAM, BlockManagerId(driver, task4-fa4e6d961dba3a3d-driver-svc.data-science-ec24848.svc, 7079, None)
2025-04-10 03:24:40,133 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, task4-fa4e6d961dba3a3d-driver-svc.data-science-ec24848.svc, 7079, None)
2025-04-10 03:24:40,135 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, task4-fa4e6d961dba3a3d-driver-svc.data-science-ec24848.svc, 7079, None)
2025-04-10 03:24:40,156 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74061a74{/metrics/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:40,467 INFO history.SingleEventLogFileWriter: Logging events to s3a://spark-hs-bkt-89306845-2a18-47bf-bc82-302765918961/logs-dir/spark-bd58c9fdba9c4928a5042e129634a39b.inprogress
2025-04-10 03:24:43,647 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-10 03:24:44,281 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.132.81.227:44310) with ID 1
2025-04-10 03:24:44,283 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.132.20.18:46614) with ID 2
2025-04-10 03:24:44,323 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2025-04-10 03:24:44,380 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.132.81.227:36025 with 2.1 GiB RAM, BlockManagerId(1, 10.132.81.227, 36025, None)
2025-04-10 03:24:44,392 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.132.20.18:37549 with 2.1 GiB RAM, BlockManagerId(2, 10.132.20.18, 37549, None)
2025-04-10 03:24:44,529 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/work-dir/spark-warehouse').
2025-04-10 03:24:44,530 INFO internal.SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
2025-04-10 03:24:44,546 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c88b737{/SQL,null,AVAILABLE,@Spark}
2025-04-10 03:24:44,547 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ba72569{/SQL/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:44,547 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@abd8bff{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-10 03:24:44,548 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@321879ca{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-10 03:24:44,549 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@791f31ff{/static/sql,null,AVAILABLE,@Spark}
Using host: stream-emulator-hdfs.data-science-tools.svc.cluster.local
Using port: 5552
Question 5
-------------------------------------------
Batch: 0
-------------------------------------------
+--------+-----------+
|hostname|total_bytes|
+--------+-----------+
+--------+-----------+

-------------------------------------------
Batch: 1
-------------------------------------------
+-------------+-----------+
|hostname     |total_bytes|
+-------------+-----------+
|             |1246719    |
|10.251.215.16|91178      |
|10.250.14.224|91178      |
|10.250.19.102|null       |
|10.250.10.6  |null       |
|10.251.71.16 |null       |
+-------------+-----------+

-------------------------------------------
Batch: 2
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2188581    |
|10.251.215.16 |91178      |
|10.250.14.224 |91178      |
|10.250.19.102 |null       |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
|10.250.10.6   |null       |
|10.251.197.226|null       |
+--------------+-----------+

-------------------------------------------
Batch: 3
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2272414    |
|10.251.215.16 |182356     |
|10.251.74.79  |91178      |
|10.250.14.224 |91178      |
|10.250.19.102 |null       |
|10.250.7.244  |null       |
|10.250.10.6   |null       |
|10.251.71.16  |null       |
|10.251.197.226|null       |
|10.250.11.100 |null       |
|10.250.14.196 |null       |
+--------------+-----------+

-------------------------------------------
Batch: 4
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2454770    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.250.14.224 |91178      |
|10.250.11.100 |null       |
|10.251.31.5   |null       |
|10.251.71.16  |null       |
|10.251.197.226|null       |
|10.250.10.6   |null       |
|10.250.14.196 |null       |
|10.250.7.244  |null       |
|10.250.19.102 |null       |
+--------------+-----------+

-------------------------------------------
Batch: 5
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2454770    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.250.14.224 |91178      |
|10.250.19.102 |null       |
|10.250.7.244  |null       |
|10.250.10.6   |null       |
|10.251.197.226|null       |
|10.250.14.196 |null       |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
|10.251.31.5   |null       |
+--------------+-----------+

-------------------------------------------
Batch: 6
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2454770    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.251.31.5   |91178      |
|10.250.14.224 |91178      |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
|10.251.197.226|null       |
|10.250.10.6   |null       |
|10.250.14.196 |null       |
|10.250.7.244  |null       |
|10.250.19.102 |null       |
+--------------+-----------+

-------------------------------------------
Batch: 7
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2454770    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.251.31.5   |91178      |
|10.250.14.224 |91178      |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
|10.251.197.226|null       |
|10.250.10.6   |null       |
|10.250.14.196 |null       |
|10.250.7.244  |null       |
|10.250.19.102 |null       |
+--------------+-----------+

-------------------------------------------
Batch: 8
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2637126    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.251.31.5   |91178      |
|10.250.14.224 |91178      |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
|10.251.197.226|null       |
|10.250.10.6   |null       |
|10.250.14.196 |null       |
|10.250.19.102 |null       |
|10.250.7.244  |null       |
+--------------+-----------+

-------------------------------------------
Batch: 9
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2637126    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.250.14.224 |91178      |
|10.251.31.5   |91178      |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
|10.251.197.226|null       |
|10.250.10.6   |null       |
|10.250.14.196 |null       |
|10.250.7.244  |null       |
|10.250.19.102 |null       |
+--------------+-----------+

-------------------------------------------
Batch: 10
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2637126    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.250.14.224 |91178      |
|10.251.31.5   |91178      |
|10.250.19.102 |null       |
|10.251.126.227|null       |
|10.250.7.244  |null       |
|10.251.126.5  |null       |
|10.251.30.134 |null       |
|10.251.197.226|null       |
|10.250.14.196 |null       |
|10.251.125.193|null       |
|10.251.203.149|null       |
|10.251.74.227 |null       |
|10.251.127.191|null       |
|10.250.18.114 |null       |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
+--------------+-----------+
only showing top 20 rows

-------------------------------------------
Batch: 11
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2637126    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.250.14.224 |91178      |
|10.251.31.5   |91178      |
|10.250.18.114 |null       |
|10.251.203.149|null       |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
|10.251.30.134 |null       |
|10.251.197.226|null       |
|10.250.10.6   |null       |
|10.251.127.191|null       |
|10.250.14.196 |null       |
|10.251.125.193|null       |
|10.251.74.227 |null       |
|10.250.19.102 |null       |
|10.251.126.227|null       |
|10.251.126.5  |null       |
+--------------+-----------+
only showing top 20 rows

-------------------------------------------
Batch: 12
-------------------------------------------
+--------------+-----------+
|hostname      |total_bytes|
+--------------+-----------+
|              |2637126    |
|10.251.215.16 |182356     |
|10.251.107.19 |91178      |
|10.251.74.79  |91178      |
|10.250.14.224 |91178      |
|10.251.31.5   |91178      |
|10.250.18.114 |null       |
|10.251.203.149|null       |
|10.250.11.100 |null       |
|10.251.71.16  |null       |
|10.251.30.134 |null       |
|10.251.197.226|null       |
|10.250.10.6   |null       |
|10.251.127.191|null       |
|10.250.14.196 |null       |
|10.251.125.193|null       |
|10.251.74.227 |null       |
|10.250.19.102 |null       |
|10.251.126.227|null       |
|10.250.7.244  |null       |
+--------------+-----------+
only showing top 20 rows

2025-04-10 03:29:47,914 ERROR v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3869d444 is aborting.
2025-04-10 03:29:47,914 ERROR v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3869d444 aborted.
2025-04-10 03:29:47,997 ERROR server.TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:169)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:684)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:253)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:750)
