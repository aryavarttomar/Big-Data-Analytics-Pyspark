++ id -u
+ myuid=1000770000
++ id -g
+ mygid=0
+ set +e
++ getent passwd 1000770000
+ uidentry='1000770000:x:1000770000:0:1000770000 user:/home/jboss:/sbin/nologin'
+ set -e
+ '[' -z '1000770000:x:1000770000:0:1000770000 user:/home/jboss:/sbin/nologin' ']'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ sort -t_ -k4 -n
+ grep SPARK_JAVA_OPT_
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/opt/hadoop/share/hadoop/tools/lib/*' ']'
+ SPARK_CLASSPATH=':/opt/spark/jars/*:/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/opt/hadoop/share/hadoop/tools/lib/*'
+ '[' '' == 2 ']'
+ '[' '' == 3 ']'
+ '[' -n /opt/hadoop ']'
+ '[' -z '/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar:/opt/hadoop/share/hadoop/tools/lib/*' ']'
+ '[' -z ']'
+ case "$1" in
+ shift 1
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.133.7.253 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner s3a://object-bucket-ec24848-8bb6712d-c99d-4727-a9d6-d67ec41345d8/spark-hs/spark-upload-73b5015c-50f1-44e1-81de-e6bb7d49eb72/task4.py
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.hadoop#hadoop-aws added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-dd9bbb96-db8a-40a7-99b6-84ae472cdb88;1.0
	confs: [default]
	found org.apache.hadoop#hadoop-aws;3.2.2 in central
	found com.amazonaws#aws-java-sdk-bundle;1.11.563 in central
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.2/hadoop-aws-3.2.2.jar ...
	[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.2!hadoop-aws.jar (39ms)
downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar ...
	[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.563!aws-java-sdk-bundle.jar (1253ms)
:: resolution report :: resolve 1289ms :: artifacts dl 1295ms
	:: modules in use:
	com.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]
	org.apache.hadoop#hadoop-aws;3.2.2 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-dd9bbb96-db8a-40a7-99b6-84ae472cdb88
	confs: [default]
	2 artifacts copied, 0 already retrieved (127385kB/68ms)
2025-04-10 03:14:20,622 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-10 03:14:23,146 INFO spark.SparkContext: Running Spark version 3.0.1
2025-04-10 03:14:23,191 INFO resource.ResourceUtils: ==============================================================
2025-04-10 03:14:23,193 INFO resource.ResourceUtils: Resources for spark.driver:

2025-04-10 03:14:23,193 INFO resource.ResourceUtils: ==============================================================
2025-04-10 03:14:23,193 INFO spark.SparkContext: Submitted application: StreamingHDFSLogs_Task
2025-04-10 03:14:23,265 INFO spark.SecurityManager: Changing view acls to: 1000770000,ec24848
2025-04-10 03:14:23,265 INFO spark.SecurityManager: Changing modify acls to: 1000770000,ec24848
2025-04-10 03:14:23,265 INFO spark.SecurityManager: Changing view acls groups to: 
2025-04-10 03:14:23,265 INFO spark.SecurityManager: Changing modify acls groups to: 
2025-04-10 03:14:23,265 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1000770000, ec24848); groups with view permissions: Set(); users  with modify permissions: Set(1000770000, ec24848); groups with modify permissions: Set()
2025-04-10 03:14:23,574 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.
2025-04-10 03:14:23,609 INFO spark.SparkEnv: Registering MapOutputTracker
2025-04-10 03:14:23,655 INFO spark.SparkEnv: Registering BlockManagerMaster
2025-04-10 03:14:23,678 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-10 03:14:23,679 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-04-10 03:14:23,683 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
2025-04-10 03:14:23,699 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-128d95c5-2d7f-42bf-b5be-facf203ae5e5/blockmgr-0c75bfe6-77f5-4b6f-89c4-bbf9177b96fe
2025-04-10 03:14:23,728 INFO memory.MemoryStore: MemoryStore started with capacity 2004.6 MiB
2025-04-10 03:14:23,747 INFO spark.SparkEnv: Registering OutputCommitCoordinator
2025-04-10 03:14:23,855 INFO util.log: Logging initialized @7557ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-10 03:14:23,936 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_332-b09
2025-04-10 03:14:23,959 INFO server.Server: Started @7661ms
2025-04-10 03:14:24,003 INFO server.AbstractConnector: Started ServerConnector@6be1a2ab{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2025-04-10 03:14:24,003 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
2025-04-10 03:14:24,033 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f531d22{/jobs,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,036 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bcd84d1{/jobs/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,036 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36ee396b{/jobs/job,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,037 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d4887af{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,038 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7aac2442{/stages,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,038 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@340eaf0{/stages/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,039 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54739ba6{/stages/stage,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,040 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f957489{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,041 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58aa66{/stages/pool,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,041 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7839bd0e{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,042 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52c311db{/storage,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,043 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4efbb456{/storage/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,043 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f9c164c{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,044 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a70ecbb{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,044 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c94e9ba{/environment,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,045 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d8c5f29{/environment/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,046 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48ea818f{/executors,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,046 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64027845{/executors/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,047 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@483b95c1{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,049 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24d7c7ac{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,059 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68ce7c24{/static,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,060 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@472f7f1d{/,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,061 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41451f25{/api,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,062 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@356b0874{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,062 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2dc8dbdb{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-10 03:14:24,065 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://task4-0949db961db0d8c3-driver-svc.data-science-ec24848.svc:4040
2025-04-10 03:14:24,100 INFO spark.SparkContext: Added JAR local:///opt/spark/jars/graphframes-0.8.2-spark3.0-s_2.12.jar at file:/opt/spark/jars/graphframes-0.8.2-spark3.0-s_2.12.jar with timestamp 1744254864100
2025-04-10 03:14:24,219 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
2025-04-10 03:14:25,195 INFO k8s.ExecutorPodsAllocator: Going to request 2 executors from Kubernetes.
2025-04-10 03:14:25,211 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
2025-04-10 03:14:25,211 INFO netty.NettyBlockTransferService: Server created on task4-0949db961db0d8c3-driver-svc.data-science-ec24848.svc:7079
2025-04-10 03:14:25,213 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-10 03:14:25,223 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, task4-0949db961db0d8c3-driver-svc.data-science-ec24848.svc, 7079, None)
2025-04-10 03:14:25,229 INFO storage.BlockManagerMasterEndpoint: Registering block manager task4-0949db961db0d8c3-driver-svc.data-science-ec24848.svc:7079 with 2004.6 MiB RAM, BlockManagerId(driver, task4-0949db961db0d8c3-driver-svc.data-science-ec24848.svc, 7079, None)
2025-04-10 03:14:25,232 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, task4-0949db961db0d8c3-driver-svc.data-science-ec24848.svc, 7079, None)
2025-04-10 03:14:25,234 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, task4-0949db961db0d8c3-driver-svc.data-science-ec24848.svc, 7079, None)
2025-04-10 03:14:25,258 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@428b4c4d{/metrics/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:25,589 INFO history.SingleEventLogFileWriter: Logging events to s3a://spark-hs-bkt-89306845-2a18-47bf-bc82-302765918961/logs-dir/spark-bea4d631f8d4425b992cd1f82a5eae5c.inprogress
2025-04-10 03:14:28,126 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-10 03:14:28,616 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.132.81.218:47742) with ID 1
2025-04-10 03:14:28,738 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.132.81.218:38711 with 2.1 GiB RAM, BlockManagerId(1, 10.132.81.218, 38711, None)
2025-04-10 03:14:29,232 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.132.20.15:36582) with ID 2
2025-04-10 03:14:29,329 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2025-04-10 03:14:29,347 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.132.20.15:38963 with 2.1 GiB RAM, BlockManagerId(2, 10.132.20.15, 38963, None)
2025-04-10 03:14:29,551 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/work-dir/spark-warehouse').
2025-04-10 03:14:29,551 INFO internal.SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
2025-04-10 03:14:29,567 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50c96fde{/SQL,null,AVAILABLE,@Spark}
2025-04-10 03:14:29,568 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@336b6928{/SQL/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:29,569 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4da43f8a{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-10 03:14:29,569 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fe3405c{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-10 03:14:29,571 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6657ab4e{/static/sql,null,AVAILABLE,@Spark}
Using host: stream-emulator-hdfs.data-science-tools.svc.cluster.local
Using port: 5552
Question 4
-------------------------------------------
Batch: 0
-------------------------------------------
+------+--------------+
|window|datanode_count|
+------+--------------+
+------+--------------+

-------------------------------------------
Batch: 1
-------------------------------------------
+------------------------------------------+--------------+
|window                                    |datanode_count|
+------------------------------------------+--------------+
|[2008-11-09 20:34:30, 2008-11-09 20:35:30]|12            |
|[2008-11-09 20:35:00, 2008-11-09 20:36:00]|12            |
+------------------------------------------+--------------+

-------------------------------------------
Batch: 2
-------------------------------------------
+------------------------------------------+--------------+
|window                                    |datanode_count|
+------------------------------------------+--------------+
|[2008-11-09 20:34:30, 2008-11-09 20:35:30]|24            |
|[2008-11-09 20:35:00, 2008-11-09 20:36:00]|24            |
+------------------------------------------+--------------+

-------------------------------------------
Batch: 3
-------------------------------------------
+------------------------------------------+--------------+
|window                                    |datanode_count|
+------------------------------------------+--------------+
|[2008-11-09 20:34:30, 2008-11-09 20:35:30]|29            |
|[2008-11-09 20:35:00, 2008-11-09 20:36:00]|29            |
+------------------------------------------+--------------+

2025-04-10 03:15:33,019 ERROR v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@311a1a3a is aborting.
2025-04-10 03:15:33,020 ERROR v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@311a1a3a aborted.
